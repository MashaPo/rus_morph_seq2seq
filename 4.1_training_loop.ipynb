{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, model_from_json, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from time import time\n",
    "#from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vectorize(df):\n",
    "    eng_sentences = list(df['x'])\n",
    "    fra_sentences = list(df['y'])\n",
    "    #print(eng_sentences[:5])\n",
    "    nb_samples = df.shape[0]\n",
    "    tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
    "    tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
    "    target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
    "    # Vectorize the english and french sentences\n",
    "\n",
    "    for i in tqdm(range(nb_samples)):\n",
    "        for k,ch in enumerate(eng_sentences[i]):\n",
    "            tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
    "\n",
    "        for k,ch in enumerate(fra_sentences[i]):\n",
    "            tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
    "\n",
    "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "            if k > 0:\n",
    "                target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
    "    return tokenized_eng_sentences, tokenized_fra_sentences, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(seq):\n",
    "    tokenized_eng_sentence = np.zeros(shape = (1,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
    "    for k,ch in enumerate(seq):\n",
    "        tokenized_eng_sentence[0,k,eng_char_to_index_dict[ch]] = 1\n",
    "        \n",
    "    return tokenized_eng_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq,encoder_model_inf,decoder_model_inf):\n",
    "    \n",
    "    # Initial states value is coming from the encoder \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
    "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
    "    \n",
    "    translated_sent = ''\n",
    "    stop_condition = False\n",
    "    \n",
    "    prob = 1.0\n",
    "    while not stop_condition:\n",
    "        \n",
    "        #decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        #predict_pr = decoder_model_inf.predict_proba(x=[target_seq] + states_val)\n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        #print(decoder_out)\n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        max_val = np.max(decoder_out[0,-1,:])\n",
    "        prob *= max_val\n",
    "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
    "        translated_sent += sampled_fra_char\n",
    "        #print('{} == {}'.format(sampled_fra_char,max_val))\n",
    "        \n",
    "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "    \n",
    "    prob = prob**(1/len(translated_sent))\n",
    "    return translated_sent, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting(step, model, tokenized_eng_sentences, tokenized_fra_sentences, target_data):\n",
    "    print('fitting on {} words...'.format(tokenized_eng_sentences.shape[0]))\n",
    "    t = time()\n",
    "    model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
    "              y=target_data,\n",
    "              batch_size=64,\n",
    "              epochs=40,\n",
    "              validation_split=0.2,\n",
    "             verbose = 0)\n",
    "    #print(model.layers)\n",
    "    print('creating new inference model...')\n",
    "    # Encoder inference model\n",
    "    encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "    # Decoder inference model\n",
    "    decoder_state_input_h = Input(shape=(256,))\n",
    "    decoder_state_input_c = Input(shape=(256,))\n",
    "    decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                     initial_state=decoder_input_states)\n",
    "\n",
    "    decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "    decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "    decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                              outputs=[decoder_out] + decoder_states )\n",
    "    print('saving inference models...')\n",
    "    encoder_model_inf.save('retraining_models/encoder-{}.h5'.format(step))\n",
    "    decoder_model_inf.save('retraining_models/decoder-{}.h5'.format(step))\n",
    "    model.save('retraining_models/model-{}.h5'.format(step))\n",
    "    print('fitting took {} min'.format(round(((time()-t)/60),4)))\n",
    "    return(encoder_model_inf,decoder_model_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#на n-ной итерации берем n*10000 примеров из теста, из них добавляем 10000 к трейну\n",
    "def testing(encoder_model_inf,decoder_model_inf,train,train_rest,count,step):\n",
    "    t = time()\n",
    "    length = max((train_rest).shape[0],step*count) #сколько отрезаем для теста или берем весь\n",
    "    test = train_rest[:length]\n",
    "    print('testing on {} words...'.format(test.shape[0]))\n",
    "    predictions = []\n",
    "    probs = []\n",
    "    for row in tqdm(range(test.shape[0])):\n",
    "        x = test.iloc[row,5]\n",
    "        x_seq = tokenize(x)\n",
    "        y,prob = decode_seq(x_seq,encoder_model_inf,decoder_model_inf)\n",
    "        predictions.append(y.strip())\n",
    "        probs.append(prob)\n",
    "    test['predictions'] = predictions\n",
    "    test['probs'] = probs\n",
    "    test = test.sort_values(by=['probs'])\n",
    "    name = 'test_after_{}_step.csv'.format(step)\n",
    "    test[['x','predictions','probs']].to_csv('training_loop/test/' + name, sep='\\t', encoding = 'utf-8')\n",
    "    test = test[:count] # берем худшие count строк\n",
    "    #выкидываем из train_rest строки, которые уходят в трейн\n",
    "    valuelist = test['x']\n",
    "    train_rest = train_rest[~train_rest.x.isin(valuelist)]\n",
    "    train = test.copy()\n",
    "    name_train = 'train_rest_after_{}_step.csv'.format(step)\n",
    "    train_rest[['x']].to_csv('training_loop/test/' + name_train, sep='\\t', encoding = 'utf-8')\n",
    "    train = test.copy()\n",
    "    print('testing took {} min'.format(round(((time()-t)/60),4)))\n",
    "    return train,train_rest\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### один раз вычитываем данные,  создаем encoder decoder, компилируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train = pd.read_csv('data/train/train_full.csv', sep='\\t', encoding = 'utf-8')\n",
    "total_train['x'] = total_train.apply(lambda row: row['lemma'] + str(row['formtag']), axis=1)\n",
    "total_train['y'] ='\\t' + total_train[\"form\"] + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filtered = total_train[total_train.apply(lambda x: not x['classtag'].endswith(('-','+','0')), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pickle.load( open( \"data.p\", \"rb\" ))\n",
    "eng_chars = data['eng_chars']\n",
    "fra_chars = data['fra_chars']\n",
    "max_len_eng_sent = data['max_len_eng_sent']\n",
    "max_len_fra_sent = data['max_len_fra_sent']\n",
    "eng_index_to_char_dict = data['eng_index_to_char_dict']\n",
    "eng_char_to_index_dict = data['eng_char_to_index_dict']\n",
    "fra_index_to_char_dict = data['fra_index_to_char_dict']\n",
    "fra_char_to_index_dict = data['fra_char_to_index_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "encoder_input = Input(shape=(None,len(eng_chars)))\n",
    "encoder_LSTM = LSTM(256,return_state = True)\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
    "encoder_states = [encoder_h, encoder_c]\n",
    "# Decoder model\n",
    "decoder_input = Input(shape=(None,len(fra_chars)))\n",
    "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
    "decoder_out = decoder_dense (decoder_out)\n",
    "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
    "# Compiling\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, rest of train: 373532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 65603.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 11.3065 min\n",
      "testing on 373532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 373532/373532 [58:44<00:00, 105.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 58.793 min\n",
      "iteration: 2, rest of train: 363532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 69744.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 11.0127 min\n",
      "testing on 363532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 363532/363532 [56:15<00:00, 107.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 56.3067 min\n",
      "iteration: 3, rest of train: 353532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 62713.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 11.0465 min\n",
      "testing on 353532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 353532/353532 [55:32<00:00, 106.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 55.5924 min\n",
      "iteration: 4, rest of train: 343532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 60081.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 11.2535 min\n",
      "testing on 343532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 343532/343532 [53:44<00:00, 106.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 53.781 min\n",
      "iteration: 5, rest of train: 333532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 62334.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 11.0945 min\n",
      "testing on 333532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 333532/333532 [52:40<00:00, 105.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 52.72 min\n",
      "iteration: 6, rest of train: 323532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 64350.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 11.1417 min\n",
      "testing on 323532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 323532/323532 [51:06<00:00, 105.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 51.1573 min\n",
      "iteration: 7, rest of train: 313532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 66036.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 11.193 min\n",
      "testing on 313532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 313532/313532 [49:38<00:00, 105.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 49.6797 min\n",
      "iteration: 8, rest of train: 303532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 64751.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 11.2035 min\n",
      "testing on 303532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 303532/303532 [48:00<00:00, 105.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 48.0448 min\n",
      "iteration: 9, rest of train: 293532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 64333.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 11.2735 min\n",
      "testing on 293532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 293532/293532 [46:00<00:00, 106.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 46.0493 min\n",
      "iteration: 10, rest of train: 283532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 63932.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 11.3051 min\n",
      "testing on 283532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 283532/283532 [1:03:10<00:00, 74.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 63.2523 min\n",
      "iteration: 11, rest of train: 273532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 36135.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 15.0605 min\n",
      "testing on 273532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 273532/273532 [1:10:39<00:00, 64.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 70.7256 min\n",
      "iteration: 12, rest of train: 263532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 40215.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 14.983 min\n",
      "testing on 263532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 263532/263532 [1:03:41<00:00, 68.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 63.7543 min\n",
      "iteration: 13, rest of train: 253532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 42621.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 14.8766 min\n",
      "testing on 253532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 253532/253532 [1:01:22<00:00, 68.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 61.4321 min\n",
      "iteration: 14, rest of train: 243532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 39734.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 15.0706 min\n",
      "testing on 243532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 243532/243532 [58:25<00:00, 69.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 58.4829 min\n",
      "iteration: 15, rest of train: 233532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 35366.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 15.0255 min\n",
      "testing on 233532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 233532/233532 [54:52<00:00, 70.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 54.9342 min\n",
      "iteration: 16, rest of train: 223532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 43362.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 14.7046 min\n",
      "testing on 223532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 223532/223532 [50:39<00:00, 73.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 50.7169 min\n",
      "iteration: 17, rest of train: 213532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 42988.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 14.5336 min\n",
      "testing on 213532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 213532/213532 [47:35<00:00, 74.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 47.6401 min\n",
      "iteration: 18, rest of train: 203532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 44723.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 14.6142 min\n",
      "testing on 203532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 203532/203532 [46:44<00:00, 72.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 46.7949 min\n",
      "iteration: 19, rest of train: 193532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 39577.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 13.9151 min\n",
      "testing on 193532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 193532/193532 [30:52<00:00, 104.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 30.9071 min\n",
      "iteration: 20, rest of train: 183532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 63525.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 11.9949 min\n",
      "testing on 183532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 183532/183532 [28:47<00:00, 106.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 28.8164 min\n",
      "iteration: 21, rest of train: 173532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 66489.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 11.9826 min\n",
      "testing on 173532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 173532/173532 [27:21<00:00, 105.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 27.3767 min\n",
      "iteration: 22, rest of train: 163532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 68297.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.0422 min\n",
      "testing on 163532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 163532/163532 [26:41<00:00, 102.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 26.717 min\n",
      "iteration: 23, rest of train: 153532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 65615.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.0762 min\n",
      "testing on 153532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 153532/153532 [24:46<00:00, 103.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 24.7919 min\n",
      "iteration: 24, rest of train: 143532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 64344.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.0321 min\n",
      "testing on 143532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 143532/143532 [22:59<00:00, 104.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 23.0178 min\n",
      "iteration: 25, rest of train: 133532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 63526.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.0679 min\n",
      "testing on 133532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 133532/133532 [21:22<00:00, 104.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 21.3947 min\n",
      "iteration: 26, rest of train: 123532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 64757.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.0951 min\n",
      "testing on 123532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 123532/123532 [19:37<00:00, 104.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 19.6469 min\n",
      "iteration: 27, rest of train: 113532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 65185.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.1078 min\n",
      "testing on 113532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 113532/113532 [18:11<00:00, 104.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 18.2016 min\n",
      "iteration: 28, rest of train: 103532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 65606.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.1773 min\n",
      "testing on 103532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 103532/103532 [16:22<00:00, 105.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 16.387 min\n",
      "iteration: 29, rest of train: 93532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 66489.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.1877 min\n",
      "testing on 93532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 93532/93532 [14:50<00:00, 105.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 14.8553 min\n",
      "iteration: 30, rest of train: 83532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 67844.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.205 min\n",
      "testing on 83532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 83532/83532 [13:11<00:00, 105.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 13.2065 min\n",
      "iteration: 31, rest of train: 73532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 67849.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.2365 min\n",
      "testing on 73532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 73532/73532 [11:30<00:00, 106.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 11.5227 min\n",
      "iteration: 32, rest of train: 63532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 63525.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.232 min\n",
      "testing on 63532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 63532/63532 [09:50<00:00, 107.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 9.846 min\n",
      "iteration: 33, rest of train: 53532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 69260.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.2623 min\n",
      "testing on 53532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 53532/53532 [08:13<00:00, 108.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 8.238 min\n",
      "iteration: 34, rest of train: 43532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 68317.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.3197 min\n",
      "testing on 43532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 43532/43532 [06:40<00:00, 108.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 6.6838 min\n",
      "iteration: 35, rest of train: 33532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 72799.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.1597 min\n",
      "testing on 33532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 33532/33532 [04:57<00:00, 112.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 4.9639 min\n",
      "iteration: 36, rest of train: 23532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 72784.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.1382 min\n",
      "testing on 23532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 23532/23532 [03:27<00:00, 113.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 3.4569 min\n",
      "iteration: 37, rest of train: 13532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 72800.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.0941 min\n",
      "testing on 13532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13532/13532 [01:57<00:00, 115.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 1.9634 min\n",
      "iteration: 38, rest of train: 3532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 77907.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 12.1963 min\n",
      "testing on 3532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3532/3532 [00:30<00:00, 114.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 0.518 min\n"
     ]
    }
   ],
   "source": [
    "count = 10000\n",
    "train,train_rest = train_filtered[:count],train_filtered[count:]\n",
    "step = 1\n",
    "while train_rest.shape[0] > 0:\n",
    "    print('iteration: {}, rest of train: {}'.format(step, train_rest.shape[0]))\n",
    "    tokenized_eng_sentences, tokenized_fra_sentences, target_data = vectorize(train)\n",
    "    encoder_model_inf,decoder_model_inf = fitting(step, model, tokenized_eng_sentences, tokenized_fra_sentences, target_data) \n",
    "    train,train_rest = testing(encoder_model_inf,decoder_model_inf,train,train_rest,count,step)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
