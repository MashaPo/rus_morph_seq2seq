{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.5\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, model_from_json, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from time import time\n",
    "#from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.6'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fast and memory efficient implementation\n",
    "# by Hjelmqvist, Sten\n",
    "def levenshtein(s, t):\n",
    "    # degenerate cases\n",
    "    if s == t:\n",
    "        return 0\n",
    "    if len(s) == 0:\n",
    "        return len(t)\n",
    "    if len(t) == 0:\n",
    "        return len(s)\n",
    "  \n",
    "    # create two work vectors of integer distances\n",
    "    #int[] v0 = new int[t.Length + 1];\n",
    "    #int[] v1 = new int[t.Length + 1];\n",
    "    v0 = []\n",
    "    v1 = []\n",
    "  \n",
    "    # initialize v0 (the previous row of distances)\n",
    "    # this row is A[0][i]: edit distance for an empty s\n",
    "    # the distance is just the number of characters to delete from t\n",
    "    # for (int i = 0; i < v0.Length; i++)\n",
    "    # v0[i] = i;\n",
    "    for i in range(len(t)+1):\n",
    "        v0.append(i)\n",
    "        v1.append(0)\n",
    " \n",
    "    for i in range(len(s)): \n",
    "        # calculate v1 (current row distances) from the previous row v0\n",
    "        # first element of v1 is A[i+1][0]\n",
    "        # edit distance is delete (i+1) chars from s to match empty t\n",
    "        v1[0] = i + 1\n",
    "  \n",
    "        # use formula to fill in the rest of the row\n",
    "        for j in range(len(t)):\n",
    "            cost = 0 if s[i] == t[j] else 1;\n",
    "            v1[j + 1] = min(v1[j]+1, v0[j+1]+1, v0[j]+cost)\n",
    "  \n",
    "        # copy v1 (current row) to v0 (previous row) for next iteration\n",
    "        for j in range(len(t)+1):\n",
    "            v0[j] = v1[j]\n",
    "  \n",
    "    return v1[len(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vectorize(df):\n",
    "    eng_sentences = list(df['x'])\n",
    "    fra_sentences = list(df['y'])\n",
    "    #print(eng_sentences[:5])\n",
    "    nb_samples = df.shape[0]\n",
    "    tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
    "    tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
    "    target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
    "    # Vectorize the english and french sentences\n",
    "\n",
    "    for i in tqdm(range(nb_samples)):\n",
    "        for k,ch in enumerate(eng_sentences[i]):\n",
    "            tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
    "\n",
    "        for k,ch in enumerate(fra_sentences[i]):\n",
    "            tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
    "\n",
    "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "            if k > 0:\n",
    "                target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
    "    return tokenized_eng_sentences, tokenized_fra_sentences, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(seq):\n",
    "    tokenized_eng_sentence = np.zeros(shape = (1,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
    "    for k,ch in enumerate(seq):\n",
    "        tokenized_eng_sentence[0,k,eng_char_to_index_dict[ch]] = 1\n",
    "        \n",
    "    return tokenized_eng_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq,encoder_model_inf,decoder_model_inf):\n",
    "    \n",
    "    # Initial states value is coming from the encoder \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
    "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
    "    \n",
    "    translated_sent = ''\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        \n",
    "        #decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        #predict_pr = decoder_model_inf.predict_proba(x=[target_seq] + states_val)\n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        #print(decoder_out)\n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
    "        translated_sent += sampled_fra_char\n",
    "        #print('{} == {}'.format(sampled_fra_char,max_val))\n",
    "        \n",
    "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "    return translated_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting(step, model, tokenized_eng_sentences, tokenized_fra_sentences, target_data):\n",
    "    print('fitting on {} words...'.format(tokenized_eng_sentences.shape[0]))\n",
    "    t = time()\n",
    "    model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
    "              y=target_data,\n",
    "              batch_size=64,\n",
    "              epochs=40,\n",
    "              validation_split=0.2,\n",
    "             verbose = 0)\n",
    "    #print(model.layers)\n",
    "    print('creating new inference model...')\n",
    "    # Encoder inference model\n",
    "    encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "    # Decoder inference model\n",
    "    decoder_state_input_h = Input(shape=(256,))\n",
    "    decoder_state_input_c = Input(shape=(256,))\n",
    "    decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                     initial_state=decoder_input_states)\n",
    "\n",
    "    decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "    decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "    decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                              outputs=[decoder_out] + decoder_states )\n",
    "    print('saving inference models...')\n",
    "    encoder_model_inf.save('retraining_models/lev/encoder-{}.h5'.format(step))\n",
    "    decoder_model_inf.save('retraining_models/lev/decoder-{}.h5'.format(step))\n",
    "    model.save('retraining_models/lev/model-{}.h5'.format(step))\n",
    "    print('fitting took {} min'.format(round(((time()-t)/60),4)))\n",
    "    return(encoder_model_inf,decoder_model_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#на n-ной итерации берем n*10000 примеров из теста, из них добавляем 10000 к трейну\n",
    "def testing(encoder_model_inf,decoder_model_inf,train,train_rest,count,step):\n",
    "    t = time()\n",
    "    length = max((train_rest).shape[0],step*count) #сколько отрезаем для теста или берем весь\n",
    "    test = train_rest[:length]\n",
    "    print('testing on {} words...'.format(test.shape[0]))\n",
    "    predictions = []\n",
    "    dists = []\n",
    "    for row in tqdm(range(test.shape[0])):\n",
    "        x = test.iloc[row,5]\n",
    "        x_seq = tokenize(x)\n",
    "        y = decode_seq(x_seq,encoder_model_inf,decoder_model_inf)\n",
    "        prediction = y.strip()\n",
    "        predictions.append(prediction)\n",
    "        true_form = test.iloc[row,4]\n",
    "        dist = levenshtein(prediction,true_form)\n",
    "        dists.append(dist)\n",
    "    test['predictions'] = predictions\n",
    "    test['dists'] = dists\n",
    "    test = test.sort_values(by=['dists'], ascending = False)\n",
    "    name = 'test_after_{}_step.csv'.format(step)\n",
    "    test[['x','predictions','dists']].to_csv('training_loop/test/lev/' + name, sep='\\t', encoding = 'utf-8')\n",
    "    test = test[:count] # берем худшие count строк\n",
    "    #выкидываем из train_rest строки, которые уходят в трейн\n",
    "    valuelist = test['x']\n",
    "    train_rest = train_rest[~train_rest.x.isin(valuelist)]\n",
    "    train = test.copy()\n",
    "    name_train = 'train_rest_after_{}_step.csv'.format(step)\n",
    "    train_rest[['x']].to_csv('training_loop/test/lev/' + name_train, sep='\\t', encoding = 'utf-8')\n",
    "    train = test.copy()\n",
    "    print('testing took {} min'.format(round(((time()-t)/60),4)))\n",
    "    return train,train_rest\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### один раз вычитываем данные,  создаем encoder decoder, компилируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train = pd.read_csv('data/train/train_full.csv', sep='\\t', encoding = 'utf-8')\n",
    "total_train['x'] = total_train.apply(lambda row: row['lemma'] + str(row['formtag']), axis=1)\n",
    "total_train['y'] ='\\t' + total_train[\"form\"] + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered = total_train[total_train.apply(lambda x: not x['classtag'].endswith(('-','+','0')), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509928"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42494.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "509928/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>gender</th>\n",
       "      <th>formtag</th>\n",
       "      <th>classtag</th>\n",
       "      <th>form</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>високос</td>\n",
       "      <td>м</td>\n",
       "      <td>2N</td>\n",
       "      <td>м1а</td>\n",
       "      <td>високосы</td>\n",
       "      <td>високос2N</td>\n",
       "      <td>\\tвисокосы\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>зимовщик</td>\n",
       "      <td>мо</td>\n",
       "      <td>2N</td>\n",
       "      <td>мо3а</td>\n",
       "      <td>зимовщики</td>\n",
       "      <td>зимовщик2N</td>\n",
       "      <td>\\tзимовщики\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>бензель</td>\n",
       "      <td>м</td>\n",
       "      <td>1D</td>\n",
       "      <td>м2а</td>\n",
       "      <td>бензелю</td>\n",
       "      <td>бензель1D</td>\n",
       "      <td>\\tбензелю\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>хромоскоп</td>\n",
       "      <td>м</td>\n",
       "      <td>1A</td>\n",
       "      <td>м1а</td>\n",
       "      <td>хромоскоп</td>\n",
       "      <td>хромоскоп1A</td>\n",
       "      <td>\\tхромоскоп\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>дерматология</td>\n",
       "      <td>ж</td>\n",
       "      <td>1L</td>\n",
       "      <td>ж7а</td>\n",
       "      <td>дерматологии</td>\n",
       "      <td>дерматология1L</td>\n",
       "      <td>\\tдерматологии\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lemma gender formtag classtag          form               x  \\\n",
       "0       високос      м      2N      м1а      високосы       високос2N   \n",
       "1      зимовщик     мо      2N     мо3а     зимовщики      зимовщик2N   \n",
       "2       бензель      м      1D      м2а       бензелю       бензель1D   \n",
       "3     хромоскоп      м      1A      м1а     хромоскоп     хромоскоп1A   \n",
       "4  дерматология      ж      1L      ж7а  дерматологии  дерматология1L   \n",
       "\n",
       "                  y  \n",
       "0      \\tвисокосы\\n  \n",
       "1     \\tзимовщики\\n  \n",
       "2       \\tбензелю\\n  \n",
       "3     \\tхромоскоп\\n  \n",
       "4  \\tдерматологии\\n  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filtered[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load( open( \"data.p\", \"rb\" ))\n",
    "eng_chars = data['eng_chars']\n",
    "fra_chars = data['fra_chars']\n",
    "max_len_eng_sent = data['max_len_eng_sent']\n",
    "max_len_fra_sent = data['max_len_fra_sent']\n",
    "eng_index_to_char_dict = data['eng_index_to_char_dict']\n",
    "eng_char_to_index_dict = data['eng_char_to_index_dict']\n",
    "fra_index_to_char_dict = data['fra_index_to_char_dict']\n",
    "fra_char_to_index_dict = data['fra_char_to_index_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "encoder_input = Input(shape=(None,len(eng_chars)))\n",
    "encoder_LSTM = LSTM(256,return_state = True)\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
    "encoder_states = [encoder_h, encoder_c]\n",
    "# Decoder model\n",
    "decoder_input = Input(shape=(None,len(fra_chars)))\n",
    "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
    "decoder_out = decoder_dense (decoder_out)\n",
    "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
    "# Compiling\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, rest of train: 373532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 49746.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 9.6806 min\n",
      "testing on 373532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 373532/373532 [1:12:21<00:00, 86.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 72.4239 min\n",
      "iteration: 2, rest of train: 363532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 30711.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 9.4798 min\n",
      "testing on 363532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 363532/363532 [1:08:21<00:00, 88.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 68.4268 min\n",
      "iteration: 3, rest of train: 353532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 47075.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 9.5308 min\n",
      "testing on 353532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 353532/353532 [1:07:53<00:00, 86.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 67.947 min\n",
      "iteration: 4, rest of train: 343532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 39776.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 9.5909 min\n",
      "testing on 343532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 343532/343532 [1:03:56<00:00, 89.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 63.9932 min\n",
      "iteration: 5, rest of train: 333532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 55430.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 9.5425 min\n",
      "testing on 333532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 333532/333532 [1:02:55<00:00, 88.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 62.9737 min\n",
      "iteration: 6, rest of train: 323532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 46815.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.6143 min\n",
      "testing on 323532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 323532/323532 [54:13<00:00, 99.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 54.2684 min\n",
      "iteration: 7, rest of train: 313532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 60458.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.6353 min\n",
      "testing on 313532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 313532/313532 [52:45<00:00, 99.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 52.8115 min\n",
      "iteration: 8, rest of train: 303532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 46903.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 11.7574 min\n",
      "testing on 303532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 303532/303532 [52:01<00:00, 97.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 52.0751 min\n",
      "iteration: 9, rest of train: 293532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 50656.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.6818 min\n",
      "testing on 293532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 293532/293532 [49:09<00:00, 99.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 49.2075 min\n",
      "iteration: 10, rest of train: 283532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 50863.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.6818 min\n",
      "testing on 283532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 283532/283532 [47:23<00:00, 99.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 47.4289 min\n",
      "iteration: 11, rest of train: 273532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 54944.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.2581 min\n",
      "testing on 273532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 273532/273532 [45:43<00:00, 99.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 45.7713 min\n",
      "iteration: 12, rest of train: 263532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 50863.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.1812 min\n",
      "testing on 263532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 263532/263532 [44:00<00:00, 99.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 44.0492 min\n",
      "iteration: 13, rest of train: 253532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 55247.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.1937 min\n",
      "testing on 253532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 253532/253532 [42:23<00:00, 99.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 42.4267 min\n",
      "iteration: 14, rest of train: 243532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 46946.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.2215 min\n",
      "testing on 243532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 243532/243532 [40:42<00:00, 99.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 40.7369 min\n",
      "iteration: 15, rest of train: 233532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 51438.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.2176 min\n",
      "testing on 233532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 233532/233532 [39:07<00:00, 99.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 39.1573 min\n",
      "iteration: 16, rest of train: 223532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 45954.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.1962 min\n",
      "testing on 223532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 223532/223532 [37:23<00:00, 99.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 37.4333 min\n",
      "iteration: 17, rest of train: 213532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 51280.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.2148 min\n",
      "testing on 213532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 213532/213532 [35:53<00:00, 99.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 35.9239 min\n",
      "iteration: 18, rest of train: 203532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 44324.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.2598 min\n",
      "testing on 203532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 203532/203532 [34:08<00:00, 99.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 34.1793 min\n",
      "iteration: 19, rest of train: 193532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 54582.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.255 min\n",
      "testing on 193532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 193532/193532 [32:22<00:00, 99.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 32.413 min\n",
      "iteration: 20, rest of train: 183532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 47526.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.3054 min\n",
      "testing on 183532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 183532/183532 [30:46<00:00, 99.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 30.7988 min\n",
      "iteration: 21, rest of train: 173532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 46946.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.3171 min\n",
      "testing on 173532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 173532/173532 [29:00<00:00, 99.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 29.0283 min\n",
      "iteration: 22, rest of train: 163532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 47168.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.2104 min\n",
      "testing on 163532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 163532/163532 [27:20<00:00, 99.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 27.3712 min\n",
      "iteration: 23, rest of train: 153532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 58274.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.2408 min\n",
      "testing on 153532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 153532/153532 [25:35<00:00, 100.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 25.6113 min\n",
      "iteration: 24, rest of train: 143532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 58291.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 8.3439 min\n",
      "testing on 143532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 143532/143532 [28:36<00:00, 83.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 28.6375 min\n",
      "iteration: 25, rest of train: 133532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 37340.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.3011 min\n",
      "testing on 133532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 133532/133532 [29:00<00:00, 76.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 29.037 min\n",
      "iteration: 26, rest of train: 123532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 37063.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.303 min\n",
      "testing on 123532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 123532/123532 [26:48<00:00, 76.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 26.8398 min\n",
      "iteration: 27, rest of train: 113532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 37256.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.353 min\n",
      "testing on 113532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 113532/113532 [24:37<00:00, 76.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 24.648 min\n",
      "iteration: 28, rest of train: 103532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 40616.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.3054 min\n",
      "testing on 103532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 103532/103532 [22:25<00:00, 76.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 22.4534 min\n",
      "iteration: 29, rest of train: 93532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 36309.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.3332 min\n",
      "testing on 93532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 93532/93532 [20:21<00:00, 76.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 20.3798 min\n",
      "iteration: 30, rest of train: 83532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 37964.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.4763 min\n",
      "testing on 83532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 83532/83532 [18:10<00:00, 76.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 18.1915 min\n",
      "iteration: 31, rest of train: 73532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 37621.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.4457 min\n",
      "testing on 73532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 73532/73532 [16:02<00:00, 76.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 16.0536 min\n",
      "iteration: 32, rest of train: 63532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 44522.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.4789 min\n",
      "testing on 63532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 63532/63532 [13:50<00:00, 76.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 13.865 min\n",
      "iteration: 33, rest of train: 53532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 37480.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.4396 min\n",
      "testing on 53532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 53532/53532 [11:37<00:00, 76.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 11.6395 min\n",
      "iteration: 34, rest of train: 43532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 38490.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.4591 min\n",
      "testing on 43532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 43532/43532 [09:27<00:00, 76.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 9.4679 min\n",
      "iteration: 35, rest of train: 33532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 44522.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.4378 min\n",
      "testing on 33532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 33532/33532 [07:16<00:00, 76.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 7.2805 min\n",
      "iteration: 36, rest of train: 23532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 38254.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.4518 min\n",
      "testing on 23532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 23532/23532 [05:06<00:00, 76.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 5.1118 min\n",
      "iteration: 37, rest of train: 13532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 44013.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.4885 min\n",
      "testing on 13532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 13532/13532 [02:55<00:00, 76.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 2.938 min\n",
      "iteration: 38, rest of train: 3532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 44602.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting on 10000 words...\n",
      "creating new inference model...\n",
      "saving inference models...\n",
      "fitting took 10.5383 min\n",
      "testing on 3532 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3532/3532 [00:46<00:00, 76.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing took 0.7785 min\n"
     ]
    }
   ],
   "source": [
    "count = 10000\n",
    "train,train_rest = train_filtered[:count],train_filtered[count:]\n",
    "step = 1\n",
    "while train_rest.shape[0] > 0:\n",
    "    print('iteration: {}, rest of train: {}'.format(step, train_rest.shape[0]))\n",
    "    tokenized_eng_sentences, tokenized_fra_sentences, target_data = vectorize(train)\n",
    "    encoder_model_inf,decoder_model_inf = fitting(step, model, tokenized_eng_sentences, tokenized_fra_sentences, target_data) \n",
    "    train,train_rest = testing(encoder_model_inf,decoder_model_inf,train,train_rest,count,step)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## testing Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/eval/eval_test_full_letters.csv', sep='\\t', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.apply(lambda x: not x['classtag'].endswith(('-','+','0')), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['lemma','formtag','form','seq2seq_predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dists = []\n",
    "for row in tqdm(range(data.shape[0])):\n",
    "    prediction = data.iloc[row,3]\n",
    "    true_form = data.iloc[row,2]\n",
    "    dist = levenshtein(prediction,true_form)\n",
    "    dists.append(dist)\n",
    "data['dist'] = dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by=['dist'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.loc[data['form'] == 'ржанцы']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
